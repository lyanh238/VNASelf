"""
OCR Agent for document processing, text extraction, and semantic search
"""

from typing import List, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_core.tools import tool
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from pathlib import Path
from html import escape
from .base_agent import BaseAgent


OUTPUT_DIR = Path(__file__).parent.parent / "output"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


class OCRAgent(BaseAgent):
    """OCR agent that provides PDF reader and Optical Character Recognition functionality."""
    
    def __init__(self, model: ChatOpenAI, timezone: str = "Asia/Ho_Chi_Minh"):
        super().__init__(model, timezone)
        self.name = "OCR Agent"
        self._tools = None
        self.vectorstore = None
        self.embeddings = None
        self.text_splitter = None
        self.ocr_pipeline = None
        self.processed_files = {}  # Track processed files
    
    async def initialize(self):
        """Initialize the OCR agent with PaddleOCR and vector store."""
        try:
            # Initialize embeddings
            self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            
            # Initialize text splitter
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=100,
                length_function=len,
            )
            
            # Try to initialize PaddleOCR
            try:
                from paddleocr import PPStructureV3
                self.ocr_pipeline = PPStructureV3(
                    use_doc_orientation_classify=False,
                    use_doc_unwarping=False
                )
                print("[OK] OCR Agent initialized with PaddleOCR")
            except ImportError:
                print("[WARNING] PaddleOCR not installed. Install with: pip install paddleocr")
                self.ocr_pipeline = None
            
            # Create tools
            self._tools = [
                self._create_ocr_tool(),
                self._create_search_document_tool(),
                self._create_list_documents_tool()
            ]
            
            print("[OK] OCR Agent initialized successfully")
            
        except Exception as e:
            print(f"[ERROR] Failed to initialize OCR Agent: {e}")
            self._tools = [self._create_mock_ocr_tool()]
    
    def _create_ocr_tool(self):
        """Create the OCR processing tool."""
        
        @tool
        async def process_document(file_path: str, file_type: str = "auto") -> str:
            """
            Process a document (PDF or image) using OCR and store it in vector database.
            
            Args:
                file_path: Path to the file to process
                file_type: Type of file ("pdf", "image", or "auto" for auto-detection)
            
            Returns:
                Status message with extracted text summary
            """
            try:
                if not self.ocr_pipeline:
                    return "L·ªói: PaddleOCR ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng c√†i ƒë·∫∑t v·ªõi: pip install paddleocr"
                
                # Validate file exists
                path = Path(file_path)
                if not path.exists():
                    return f"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n: {file_path}"
                
                # Auto-detect file type
                if file_type == "auto":
                    suffix = path.suffix.lower()
                    if suffix == ".pdf":
                        file_type = "pdf"
                    elif suffix in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
                        file_type = "image"
                    else:
                        return f"L·ªói: ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {suffix}"
                
                # Process with OCR
                output = self.ocr_pipeline.predict(input=file_path)
                
                # Extract text from OCR results
                markdown_filename = f"{path.stem}.md"
                markdown_path = OUTPUT_DIR / markdown_filename
                
                # Save results to markdown files
                for res in output:
                    res.save_to_markdown(save_path=str(OUTPUT_DIR))
                
                # Read markdown file content
                if markdown_path.exists():
                    with open(markdown_path, 'r', encoding='utf-8') as f:
                        full_text = f.read()
                else:
                    full_text = ""
                
                if not full_text.strip():
                    return f"C·∫£nh b√°o: Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ file: {path.name}"
                
                # Split text into chunks
                chunks = self.text_splitter.split_text(full_text)
                
                # Create metadata for chunks
                metadatas = [
                    {
                        "source": path.name,
                        "file_path": str(path),
                        "file_type": file_type,
                        "chunk_index": i
                    }
                    for i in range(len(chunks))
                ]
                
                # Create or update vector store
                if self.vectorstore is None:
                    self.vectorstore = FAISS.from_texts(
                        texts=chunks,
                        embedding=self.embeddings,
                        metadatas=metadatas
                    )
                else:
                    new_vectorstore = FAISS.from_texts(
                        texts=chunks,
                        embedding=self.embeddings,
                        metadatas=metadatas
                    )
                    self.vectorstore.merge_from(new_vectorstore)
                
                # Track processed file
                self.processed_files[path.name] = {
                    "path": str(path),
                    "type": file_type,
                    "chunks": len(chunks),
                    "char_count": len(full_text),
                    "markdown_file": markdown_filename
                }
                
                preview_html = self._render_markdown_preview(full_text)
                markdown_url = f"/api/ocr/markdown/{markdown_filename}"
                
                # Return summary
                summary = (
                    f"<div class='ocr-summary'>"
                    f"<p>‚úÖ <strong>X·ª≠ l√Ω th√†nh c√¥ng file:</strong> {path.name}</p>"
                    f"<ul>"
                    f"<li>Lo·∫°i file: <strong>{file_type.upper()}</strong></li>"
                    f"<li>S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n: <strong>{len(chunks)}</strong></li>"
                    f"<li>T·ªïng s·ªë k√Ω t·ª±: <strong>{len(full_text):,}</strong></li>"
                    f"<li>File markdown: <strong>{markdown_filename}</strong></li>"
                    f"</ul>"
                    f"</div>"
                    f"<div class='ocr-preview'>"
                    f"<h3>üìë N·ªôi dung OCR (Markdown)</h3>"
                    f"{preview_html}"
                    f"</div>"
                    f"<div class='ocr-download'>"
                    f"üìé <a href=\"{markdown_url}\" target=\"_blank\" rel=\"noopener noreferrer\">T·∫£i file markdown v·ª´a x·ª≠ l√Ω</a>"
                    f"<br><small>Ho·∫∑c truy c·∫≠p trong th∆∞ m·ª•c output/{markdown_filename}</small>"
                    f"</div>"
                    f"<div class='ocr-tip'>"
                    f"<em>T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o c∆° s·ªü d·ªØ li·ªáu vector. B·∫°n c√≥ th·ªÉ t√¨m ki·∫øm th√¥ng tin b·∫±ng c√¥ng c·ª• search_document.</em>"
                    f"</div>"
                )
                
                return summary
                
            except Exception as e:
                return f"L·ªói khi x·ª≠ l√Ω t√†i li·ªáu: {str(e)}"
        
        return process_document
    
    def _create_search_document_tool(self):
        """Create the document search tool."""
        
        @tool
        async def search_document(query: str, max_results: int = 3) -> str:
            """
            Search for information in processed documents using semantic search.
            
            Args:
                query: The search query
                max_results: Maximum number of results to return (default: 3)
            
            Returns:
                Relevant text chunks from documents
            """
            try:
                if self.vectorstore is None:
                    return "Ch∆∞a c√≥ t√†i li·ªáu n√†o ƒë∆∞·ª£c x·ª≠ l√Ω. Vui l√≤ng s·ª≠ d·ª•ng process_document ƒë·ªÉ t·∫£i t√†i li·ªáu tr∆∞·ªõc."
                
                # Perform similarity search
                results = self.vectorstore.similarity_search_with_score(
                    query=query,
                    k=max_results
                )
                
                if not results:
                    return f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o cho truy v·∫•n: '{query}'"
                
                # Format results
                formatted_results = []
                formatted_results.append(f"üîç **K·∫øt qu·∫£ t√¨m ki·∫øm cho: '{query}'**\n")
                
                for i, (doc, score) in enumerate(results, 1):
                    source = doc.metadata.get('source', 'Kh√¥ng r√µ ngu·ªìn')
                    chunk_index = doc.metadata.get('chunk_index', 'N/A')
                    content = doc.page_content
                    
                    # Calculate relevance percentage (lower score = more relevant)
                    relevance = max(0, 100 - (score * 100))
                    
                    formatted_results.append(f"**{i}. Ngu·ªìn: {source}** (ƒêo·∫°n #{chunk_index})")
                    formatted_results.append(f"üìä ƒê·ªô li√™n quan: {relevance:.1f}%")
                    formatted_results.append(f"üìù N·ªôi dung:")
                    formatted_results.append(f"{content}\n")
                
                formatted_results.append("---")
                formatted_results.append("üí° **G·ª£i √Ω:** B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh truy v·∫•n ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c h∆°n.")
                
                return "\n".join(formatted_results)
                
            except Exception as e:
                return f"L·ªói khi t√¨m ki·∫øm: {str(e)}"
        
        return search_document
    
    def _create_list_documents_tool(self):
        """Create the list documents tool."""
        
        @tool
        async def list_documents() -> str:
            """
            List all processed documents in the system.
            
            Returns:
                List of processed documents with details
            """
            try:
                if not self.processed_files:
                    return "üìÇ Ch∆∞a c√≥ t√†i li·ªáu n√†o ƒë∆∞·ª£c x·ª≠ l√Ω."
                
                result = ["üìö **Danh s√°ch t√†i li·ªáu ƒë√£ x·ª≠ l√Ω:**\n"]
                
                for i, (filename, info) in enumerate(self.processed_files.items(), 1):
                    result.append(f"**{i}. {filename}**")
                    result.append(f"   - Lo·∫°i: {info['type'].upper()}")
                    result.append(f"   - S·ªë ƒëo·∫°n: {info['chunks']}")
                    result.append(f"   - K√Ω t·ª±: {info['char_count']:,}")
                    result.append(f"   - ƒê∆∞·ªùng d·∫´n: {info['path']}\n")
                
                result.append(f"**T·ªïng s·ªë t√†i li·ªáu:** {len(self.processed_files)}")
                
                return "\n".join(result)
                
            except Exception as e:
                return f"L·ªói khi li·ªát k√™ t√†i li·ªáu: {str(e)}"
        
        return list_documents
    
    def _create_mock_ocr_tool(self):
        """Create a mock OCR tool when dependencies are not available."""
        
        @tool
        async def mock_ocr(file_path: str) -> str:
            """
            Mock OCR function when dependencies are not available.
            
            Args:
                file_path: Path to the file
            
            Returns:
                Mock processing message
            """
            return f"""‚ö†Ô∏è **Ch·∫ø ƒë·ªô m√¥ ph·ªèng OCR**

File: {file_path}

ƒê·ªÉ s·ª≠ d·ª•ng ch·ª©c nƒÉng OCR th·ª±c, vui l√≤ng c√†i ƒë·∫∑t:
- pip install paddleocr
- pip install paddlepaddle

Sau ƒë√≥ kh·ªüi ƒë·ªông l·∫°i agent."""
        
        return mock_ocr
    
    def get_system_prompt(self) -> str:
        return """B·∫°n l√† OCR Agent chuy√™n v·ªÅ x·ª≠ l√Ω t√†i li·ªáu v√† tr√≠ch xu·∫•t vƒÉn b·∫£n.

QUY T·∫ÆC NG√îN NG·ªÆ:
- M·∫∑c ƒë·ªãnh tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
- N·∫øu ng∆∞·ªùi d√πng h·ªèi b·∫±ng ng√¥n ng·ªØ kh√°c, tr·∫£ l·ªùi b·∫±ng ch√≠nh ng√¥n ng·ªØ ƒë√≥.

NHI·ªÜM V·ª§:
- X·ª≠ l√Ω PDF v√† h√¨nh ·∫£nh b·∫±ng OCR (Optical Character Recognition)
- Tr√≠ch xu·∫•t vƒÉn b·∫£n t·ª´ t√†i li·ªáu
- L∆∞u tr·ªØ vƒÉn b·∫£n v√†o c∆° s·ªü d·ªØ li·ªáu vector
- T√¨m ki·∫øm ng·ªØ nghƒ©a trong t√†i li·ªáu ƒë√£ x·ª≠ l√Ω
- Qu·∫£n l√Ω danh s√°ch t√†i li·ªáu

C√îNG C·ª§ AVAILABLE:
1. process_document: X·ª≠ l√Ω file PDF ho·∫∑c ·∫£nh, tr√≠ch xu·∫•t vƒÉn b·∫£n v√† l∆∞u v√†o vector database
2. search_document: T√¨m ki·∫øm th√¥ng tin trong c√°c t√†i li·ªáu ƒë√£ x·ª≠ l√Ω
3. list_documents: Li·ªát k√™ t·∫•t c·∫£ t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω

QUY TR√åNH X·ª¨ L√ù T√ÄI LI·ªÜU:
1. Nh·∫≠n file t·ª´ ng∆∞·ªùi d√πng (PDF ho·∫∑c ·∫£nh)
2. S·ª≠ d·ª•ng process_document ƒë·ªÉ tr√≠ch xu·∫•t vƒÉn b·∫£n
3. VƒÉn b·∫£n ƒë∆∞·ª£c chia th√†nh c√°c ƒëo·∫°n nh·ªè (chunks)
4. M·ªói ƒëo·∫°n ƒë∆∞·ª£c embedding v√† l∆∞u v√†o FAISS vector store
5. File markdown ƒë∆∞·ª£c l∆∞u t·∫°i th∆∞ m·ª•c output/

QUY TR√åNH T√åM KI·∫æM:
1. Nh·∫≠n truy v·∫•n t·ª´ ng∆∞·ªùi d√πng
2. S·ª≠ d·ª•ng search_document v·ªõi truy v·∫•n
3. T√¨m ki·∫øm ng·ªØ nghƒ©a trong vector database
4. Tr·∫£ v·ªÅ c√°c ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t
5. Hi·ªÉn th·ªã ngu·ªìn v√† ƒë·ªô li√™n quan

L∆ØU √ù:
- H·ªó tr·ª£ ƒë·ªãnh d·∫°ng: PDF, JPG, PNG, BMP, TIFF
- VƒÉn b·∫£n ƒë∆∞·ª£c chia th√†nh chunks 500 k√Ω t·ª± v·ªõi overlap 100 k√Ω t·ª±
- S·ª≠ d·ª•ng OpenAI embeddings (text-embedding-3-small)
- Vector store: FAISS cho t√¨m ki·∫øm nhanh
- Lu√¥n hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu khi tr·∫£ v·ªÅ k·∫øt qu·∫£
- N·∫øu kh√¥ng t√¨m th·∫•y, g·ª£i √Ω ng∆∞·ªùi d√πng ƒëi·ªÅu ch·ªânh truy v·∫•n"""
    
    def get_tools(self) -> List[Any]:
        """Get available tools for this agent."""
        if self._tools is None:
            raise RuntimeError("OCR agent not initialized. Call initialize() first.")
        return self._tools

    def _render_markdown_preview(self, markdown_text: str, max_chars: int = 4000) -> str:
        """Return a safe HTML preview of the markdown file."""
        content = (markdown_text or "").strip()
        if not content:
            return "<em>Kh√¥ng c√≥ n·ªôi dung ƒë·ªÉ hi·ªÉn th·ªã.</em>"
        
        truncated = content
        is_truncated = False
        if len(content) > max_chars:
            truncated = content[:max_chars]
            is_truncated = True
        
        escaped = escape(truncated)
        escaped = escaped.replace("\n", "<br>")
        
        warning = ""
        if is_truncated:
            warning = "<br><em>...(ƒê√£ r√∫t g·ªçn, h√£y t·∫£i file markdown ƒë·∫ßy ƒë·ªß ƒë·ªÉ xem to√†n b·ªô n·ªôi dung.)</em>"
        
        return (
            "<pre style=\"white-space: pre-wrap; background: #111827; color: #f3f4f6; padding: 12px; "
            "border-radius: 8px; border: 1px solid #1f2937; max-height: 420px; overflow-y: auto;\">"
            f"{escaped}{warning}"
            "</pre>"
        )